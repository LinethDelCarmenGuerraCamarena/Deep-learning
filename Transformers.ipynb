{"cells":[{"cell_type":"markdown","id":"41b7905f-a070-4ffe-abfc-67fbcd2adaa9","metadata":{"id":"41b7905f-a070-4ffe-abfc-67fbcd2adaa9"},"source":["## Deep Learning\n","## Transformers\n","\n","#### Activity 4: Implementing a Translator\n","\n","- Objective\n","\n","To understand the Transformer Architecture by Implementing a translator.\n","\n","- Instructions\n","\n","    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n","\n","    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n","\n","    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n","  \n","- Evaluation Criteria\n","\n","    - Code Readability and Comments\n","    - Traning a translator\n","    - Translating at least 10 sentences.\n","\n","- Submission\n","\n","Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered."]},{"cell_type":"markdown","id":"17f54c65","metadata":{"heading_collapsed":true,"id":"17f54c65"},"source":["#### Script to convert csv to text file"]},{"cell_type":"markdown","id":"b12e46bf","metadata":{"id":"b12e46bf"},"source":["To start the activity, we need to convert the TSV file to a CSV file. To do this, we used Microsoft Excel to open the TSV file and then save it as a UTF-8 CSV file.\n","\n","Few params were changed in the code to make it work with the CSV file instead of the TSV file."]},{"cell_type":"code","execution_count":null,"id":"8f02c0c2","metadata":{"hidden":true,"id":"8f02c0c2"},"outputs":[],"source":["#This script requires to convert the TSV file to CSV\n","# easiest way is to open it in Calc or excel and save as csv\n","PATH = './eng-spa2024.csv'\n","import pandas as pd\n","df = pd.read_csv(PATH, header=None)"]},{"cell_type":"markdown","id":"2a531593","metadata":{"id":"2a531593"},"source":["This block of code extracts the second and fourth columns from the DataFrame `df` and creates a copy of these columns. It then adds a new column named `length` that contains the length of the text in the first column. The DataFrame is sorted by this `length` column in ascending order, and the `length` column is subsequently removed. Finally, the processed DataFrame is saved to a tab-separated file named `eng-spa4.txt` without including the index and header."]},{"cell_type":"code","execution_count":null,"id":"787d9408","metadata":{"hidden":true,"id":"787d9408"},"outputs":[],"source":["eng_spa_cols = df.iloc[:, [1, 3]].copy()\n","eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n","eng_spa_cols = eng_spa_cols.sort_values(by='length')\n","eng_spa_cols = eng_spa_cols.drop(columns=['length'])\n","\n","output_file_path = './eng-spa4.txt'\n","eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"]},{"cell_type":"markdown","id":"7d468e9a","metadata":{"id":"7d468e9a"},"source":["## Transformer - Attention is all you need"]},{"cell_type":"markdown","id":"81ba2534","metadata":{"id":"81ba2534"},"source":["Importing all the necessary libraries"]},{"cell_type":"code","execution_count":null,"id":"d5dcf681","metadata":{"id":"d5dcf681","outputId":"ac074f3c-4197-47d4-d4aa-580d1f7680ad"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fc035d58030>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","import math\n","import numpy as np\n","import re\n","from tqdm import tqdm\n","\n","# Set the seed for reproducibility\n","torch.manual_seed(23)"]},{"cell_type":"markdown","id":"af1f226b","metadata":{"id":"af1f226b"},"source":["Obtaining the CUDA device if it is available"]},{"cell_type":"code","execution_count":null,"id":"2c2cbd17","metadata":{"id":"2c2cbd17","outputId":"71856db7-26cc-4b4c-8bbd-769c90330b76"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","id":"ed1ce648","metadata":{"id":"ed1ce648"},"source":["Setting the maximum sequence length for the model, this value is used to pad the sequences that are shorter than the maximum length and truncate the sequences that are longer than the maximum length."]},{"cell_type":"code","execution_count":null,"id":"9c6623a1","metadata":{"id":"9c6623a1"},"outputs":[],"source":["MAX_SEQ_LEN = 128"]},{"cell_type":"code","execution_count":null,"id":"3103d45f","metadata":{"code_folding":[30,94],"id":"3103d45f"},"outputs":[],"source":["class PositionalEmbedding(nn.Module):\n","    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n","        super().__init__()\n","        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n","        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n","                             * (-math.log(10000.0)/d_model))\n","        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n","        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n","        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n","\n","    def forward(self, x):\n","        return x + self.pos_embed_matrix[:x.size(0), :]\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model = 512, num_heads = 8):\n","        super().__init__()\n","        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n","\n","        self.d_v = d_model // num_heads\n","        self.d_k = self.d_v\n","        self.num_heads = num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def forward(self, Q, K, V, mask = None):\n","        batch_size = Q.size(0)\n","        '''\n","        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n","        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n","        '''\n","        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n","        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n","        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n","\n","        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n","        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n","        weighted_values = self.W_o(weighted_values)\n","\n","        return weighted_values, attention\n","\n","\n","    def scale_dot_product(self, Q, K, V, mask = None):\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9)\n","        attention = F.softmax(scores, dim = -1)\n","        weighted_values = torch.matmul(attention, V)\n","\n","        return weighted_values, attention\n","\n","\n","class PositionFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super().__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","\n","    def forward(self, x):\n","        return self.linear2(F.relu(self.linear1(x)))\n","\n","class EncoderSubLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n","        super().__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = PositionFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.droupout1 = nn.Dropout(dropout)\n","        self.droupout2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask = None):\n","        attention_score, _ = self.self_attn(x, x, x, mask)\n","        x = x + self.droupout1(attention_score)\n","        x = self.norm1(x)\n","        x = x + self.droupout2(self.ffn(x))\n","        return self.norm2(x)\n","\n","class Encoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.norm = nn.LayerNorm(d_model)\n","    def forward(self, x, mask=None):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","class DecoderSubLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n","        super().__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.dropout3 = nn.Dropout(dropout)\n","\n","    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n","        attention_score, _ = self.self_attn(x, x, x, target_mask)\n","        x = x + self.dropout1(attention_score)\n","        x = self.norm1(x)\n","\n","        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n","        x = x + self.dropout2(encoder_attn)\n","        x = self.norm2(x)\n","\n","        ff_output = self.feed_forward(x)\n","        x = x + self.dropout3(ff_output)\n","        return self.norm3(x)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, encoder_output, target_mask, encoder_mask):\n","        for layer in self.layers:\n","            x = layer(x, encoder_output, target_mask, encoder_mask)\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":null,"id":"61070162","metadata":{"code_folding":[],"id":"61070162"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers,\n","                 input_vocab_size, target_vocab_size,\n","                 max_len=MAX_SEQ_LEN, dropout=0.1):\n","        super().__init__()\n","        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n","        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n","        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n","        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n","        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n","        self.output_layer = nn.Linear(d_model, target_vocab_size)\n","\n","    def forward(self, source, target):\n","        # Encoder mask\n","        source_mask, target_mask = self.mask(source, target)\n","        # Embedding and positional Encoding\n","        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n","        source = self.pos_embedding(source)\n","        # Encoder\n","        encoder_output = self.encoder(source, source_mask)\n","\n","        # Decoder embedding and postional encoding\n","        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n","        target = self.pos_embedding(target)\n","        # Decoder\n","        output = self.decoder(target, encoder_output, target_mask, source_mask)\n","\n","        return self.output_layer(output)\n","\n","\n","\n","    def mask(self, source, target):\n","        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n","        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n","        size = target.size(1)\n","        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n","        target_mask = target_mask & no_mask\n","        return source_mask, target_mask\n",""]},{"cell_type":"markdown","id":"6da6b2d4","metadata":{"heading_collapsed":true,"id":"6da6b2d4"},"source":["#### Simple test"]},{"cell_type":"code","execution_count":null,"id":"d40581d6","metadata":{"hidden":true,"id":"d40581d6"},"outputs":[],"source":["seq_len_source = 10\n","seq_len_target = 10\n","batch_size = 2\n","input_vocab_size = 50\n","target_vocab_size = 50\n","\n","source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n","target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"]},{"cell_type":"code","execution_count":null,"id":"fc7cf689","metadata":{"hidden":true,"id":"fc7cf689"},"outputs":[],"source":["d_model = 512\n","num_heads = 8\n","d_ff = 2048\n","num_layers = 6\n","\n","model = Transformer(d_model, num_heads, d_ff, num_layers,\n","                  input_vocab_size, target_vocab_size,\n","                  max_len=MAX_SEQ_LEN, dropout=0.1)\n","\n","model = model.to(device)\n","source = source.to(device)\n","target = target.to(device)"]},{"cell_type":"code","execution_count":null,"id":"4618560e","metadata":{"hidden":true,"id":"4618560e"},"outputs":[],"source":["output = model(source, target)"]},{"cell_type":"code","execution_count":null,"id":"ab0bc69d","metadata":{"hidden":true,"id":"ab0bc69d","outputId":"e116d14f-ca8b-40c2-d527-0811144f3c30"},"outputs":[{"name":"stdout","output_type":"stream","text":["ouput.shape torch.Size([2, 10, 50])\n"]}],"source":["# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n","print(f'ouput.shape {output.shape}')"]},{"cell_type":"markdown","id":"0f4b2910","metadata":{"id":"0f4b2910"},"source":["### Translator Eng-Spa"]},{"cell_type":"code","execution_count":null,"id":"869a7244","metadata":{"id":"869a7244"},"outputs":[],"source":["PATH = './eng-spa4.txt'"]},{"cell_type":"code","execution_count":null,"id":"d0af1eba","metadata":{"id":"d0af1eba"},"outputs":[],"source":["with open(PATH, 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"]},{"cell_type":"code","execution_count":null,"id":"c930226f","metadata":{"id":"c930226f","outputId":"ed508579-feaf-4782-a588-975e151fa956"},"outputs":[{"data":{"text/plain":["[['Hi!', '¡Hola!'],\n"," ['Go!', '¡Sal!'],\n"," ['Go!', '¡Ya!'],\n"," ['Go!', '¡Fuera!'],\n"," ['OK.', 'Bueno.'],\n"," ['Ow!', '¡Ay!'],\n"," ['So?', '¿Y qué?'],\n"," ['Go.', 'Váyase.'],\n"," ['No.', 'No.'],\n"," ['So?', '¿Y?']]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["eng_spa_pairs[:10]"]},{"cell_type":"code","execution_count":null,"id":"095f4037","metadata":{"id":"095f4037"},"outputs":[],"source":["eng_sentences = [pair[0] for pair in eng_spa_pairs]\n","spa_sentences = [pair[1] for pair in eng_spa_pairs]"]},{"cell_type":"code","execution_count":null,"id":"0d9e1c95","metadata":{"id":"0d9e1c95","outputId":"4a1c3ac4-97c0-406f-87bf-8549a5cbf48b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Hi!', 'Go!', 'Go!', 'Go!', 'OK.', 'Ow!', 'So?', 'Go.', 'No.', 'So?']\n","['¡Hola!', '¡Sal!', '¡Ya!', '¡Fuera!', 'Bueno.', '¡Ay!', '¿Y qué?', 'Váyase.', 'No.', '¿Y?']\n"]}],"source":["print(eng_sentences[:10])\n","print(spa_sentences[:10])\n"]},{"cell_type":"code","execution_count":null,"id":"60d11478","metadata":{"id":"60d11478"},"outputs":[],"source":["def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip()\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n","    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n","    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n","    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n","    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n","    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n","    sentence = sentence.strip()\n","    sentence = '<sos> ' + sentence + ' <eos>'\n","    return sentence"]},{"cell_type":"code","execution_count":null,"id":"478f673b","metadata":{"id":"478f673b"},"outputs":[],"source":["s1 = '¿Hola @ cómo estás? 123'"]},{"cell_type":"code","execution_count":null,"id":"96ac79c5","metadata":{"id":"96ac79c5","outputId":"cbf95e06-db3e-4e49-e99c-85f6102430a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["¿Hola @ cómo estás? 123\n","<sos> hola como estas <eos>\n"]}],"source":["print(s1)\n","print(preprocess_sentence(s1))"]},{"cell_type":"code","execution_count":null,"id":"d9fc9c4d","metadata":{"id":"d9fc9c4d"},"outputs":[],"source":["eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n","spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"]},{"cell_type":"code","execution_count":null,"id":"f7a3b18d","metadata":{"id":"f7a3b18d","outputId":"376d469d-c080-41c5-fd0a-5672a67013e4"},"outputs":[{"data":{"text/plain":["['<sos> hola <eos>',\n"," '<sos> sal <eos>',\n"," '<sos> ya <eos>',\n"," '<sos> fuera <eos>',\n"," '<sos> bueno <eos>',\n"," '<sos> ay <eos>',\n"," '<sos> y que <eos>',\n"," '<sos> vayase <eos>',\n"," '<sos> no <eos>',\n"," '<sos> y <eos>']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["spa_sentences[:10]"]},{"cell_type":"code","execution_count":null,"id":"97931cd3","metadata":{"id":"97931cd3"},"outputs":[],"source":["def build_vocab(sentences):\n","    words = [word for sentence in sentences for word in sentence.split()]\n","    word_count = Counter(words)\n","    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n","    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n","    word2idx['<pad>'] = 0\n","    word2idx['<unk>'] = 1\n","    idx2word = {idx: word for word, idx in word2idx.items()}\n","    return word2idx, idx2word"]},{"cell_type":"code","execution_count":null,"id":"7fa8738e","metadata":{"id":"7fa8738e"},"outputs":[],"source":["eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n","spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n","eng_vocab_size = len(eng_word2idx)\n","spa_vocab_size = len(spa_word2idx)"]},{"cell_type":"code","execution_count":null,"id":"79d6b633","metadata":{"id":"79d6b633","outputId":"7c3fadae-8b44-4f73-923e-3396fd924d77"},"outputs":[{"name":"stdout","output_type":"stream","text":["27650 46924\n"]}],"source":["print(eng_vocab_size, spa_vocab_size)"]},{"cell_type":"code","execution_count":null,"id":"e564017c","metadata":{"id":"e564017c"},"outputs":[],"source":["class EngSpaDataset(Dataset):\n","    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n","        self.eng_sentences = eng_sentences\n","        self.spa_sentences = spa_sentences\n","        self.eng_word2idx = eng_word2idx\n","        self.spa_word2idx = spa_word2idx\n","\n","    def __len__(self):\n","        return len(self.eng_sentences)\n","\n","    def __getitem__(self, idx):\n","        eng_sentence = self.eng_sentences[idx]\n","        spa_sentence = self.spa_sentences[idx]\n","        # return tokens idxs\n","        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n","        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n","\n","        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"]},{"cell_type":"code","execution_count":null,"id":"b579577b","metadata":{"id":"b579577b"},"outputs":[],"source":["def collate_fn(batch):\n","    eng_batch, spa_batch = zip(*batch)\n","    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n","    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n","    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n","    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n","    return eng_batch, spa_batch\n",""]},{"cell_type":"code","execution_count":null,"id":"8d514b7c","metadata":{"id":"8d514b7c"},"outputs":[],"source":["def train(model, dataloader, loss_function, optimiser, epochs):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for eng_batch, spa_batch in tqdm(dataloader):\n","            eng_batch = eng_batch.to(device)\n","            spa_batch = spa_batch.to(device)\n","            # Decoder preprocessing\n","            target_input = spa_batch[:, :-1]\n","            target_output = spa_batch[:, 1:].contiguous().view(-1)\n","            # Zero grads\n","            optimiser.zero_grad()\n","            # run model\n","            output = model(eng_batch, target_input)\n","            output = output.view(-1, output.size(-1))\n","            # loss\\\n","            loss = loss_function(output, target_output)\n","            # gradient and update parameters\n","            loss.backward()\n","            optimiser.step()\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss/len(dataloader)\n","        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n","\n",""]},{"cell_type":"markdown","id":"11cd6eeb","metadata":{"id":"11cd6eeb"},"source":["Setting the batch size for the training and testing data. And initializing the dataset and dataloader for the training and testing data. The EngSpaDataset class is used to load the English-Spanish dataset using the following parameters:\n","\n","- `eng_sentences` is a list of English sentences.\n","- `spa_sentences` is a list of Spanish sentences.\n","- `eng_word2idx` is a dictionary that maps English words to their corresponding indices.\n","- `spa_word2idx` is a dictionary that maps Spanish words to their corresponding indices.\n","\n","The dataloader is used to load the dataset in batches. The batch size is set to 64, and the dataset is shuffled before each epoch."]},{"cell_type":"code","execution_count":null,"id":"2379ea72","metadata":{"id":"2379ea72"},"outputs":[],"source":["BATCH_SIZE = 64\n","dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"]},{"cell_type":"markdown","id":"1e3ca0f2","metadata":{"id":"1e3ca0f2"},"source":["Initializing a Transformer model with specific hyperparameters. The model is configured with a dimensionality of 512 for the model's hidden layers (`d_model`), 8 attention heads (`num_heads`), a feed-forward network dimension of 2048 (`d_ff`), and 6 layers (`num_layers`). The input and target vocabulary sizes are set to `eng_vocab_size` and `spa_vocab_size`, respectively, which correspond to the sizes of the English and Spanish vocabularies. The maximum sequence length is defined by `MAX_SEQ_LEN`, and a dropout rate of 0.1 is applied to prevent overfitting.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e08eef6a","metadata":{"id":"e08eef6a"},"outputs":[],"source":["model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n","                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n","                    max_len=MAX_SEQ_LEN, dropout=0.1)"]},{"cell_type":"markdown","id":"ecf34d39","metadata":{"id":"ecf34d39"},"source":["Changing the model to the CUDA device if it is available.\n","\n","As the optimizer, we are using the Adam optimizer with a learning rate of 0.0001.\n","\n","For the loss function, we are using the CrossEntropyLoss function with the parameters ignore_index=0, this parameter is used to ignore the padding token in the loss calculation."]},{"cell_type":"code","execution_count":null,"id":"a1181a12","metadata":{"id":"a1181a12"},"outputs":[],"source":["model = model.to(device)\n","loss_function = nn.CrossEntropyLoss(ignore_index=0)\n","optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"]},{"cell_type":"markdown","id":"3c9e2fc6","metadata":{"id":"3c9e2fc6"},"source":["We proceed to train the model with the dataset provided by the Tatoeba project. It's important to mention that the dataset is not very large, so the model will not be able to translate all the sentences correctly. However, the model will be able to translate some sentences correctly.\n","\n","The required computational resources to train the model are high, that's why we will use a cloud service to train the model. The model will be trained for 10 epochs using a RTX A4000 GPU with 16GB of memory, 8 CPU cores, and 48GB of RAM. The training process will take approximately 1 hour and 20 minutes.\n","\n","For reference, we used the service provided by [Paperspace](https://www.paperspace.com/)."]},{"cell_type":"code","execution_count":null,"id":"14e265e9","metadata":{"id":"14e265e9","outputId":"4eb0b095-5d50-49a2-d043-ce8e51540f21"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:08<00:00,  7.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0/10, Loss: 3.5995\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:09<00:00,  7.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1/10, Loss: 2.2074\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:07<00:00,  7.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2/10, Loss: 1.7063\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:08<00:00,  7.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3/10, Loss: 1.3786\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:08<00:00,  7.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4/10, Loss: 1.1267\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:07<00:00,  7.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5/10, Loss: 0.9234\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:07<00:00,  7.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6/10, Loss: 0.7588\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:07<00:00,  7.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7/10, Loss: 0.6298\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:08<00:00,  7.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 8/10, Loss: 0.5355\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4162/4162 [09:08<00:00,  7.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9/10, Loss: 0.4669\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Perform training, this step will take a while depending on the computation power\n","\n","train(model, dataloader, loss_function, optimiser, epochs=10)"]},{"cell_type":"markdown","id":"11e9c51b","metadata":{"id":"11e9c51b"},"source":["We added a function to save the trained model as a checkpoint file. This file will be used to load the model and translate sentences without the need to train the model again. This action is needed because the training process is computationally expensive."]},{"cell_type":"code","execution_count":null,"id":"1d271146","metadata":{"id":"1d271146"},"outputs":[],"source":["# Saving the model to disk for later use\n","\n","model_path = './transformer_eng_spa.pth'\n","torch.save(model.state_dict(), model_path)\n"]},{"cell_type":"markdown","id":"24bb8830","metadata":{"id":"24bb8830"},"source":["As well, we added a function to load the trained model from the checkpoint file. This is using the native PyTorch function `torch.load` to load the model from the checkpoint file."]},{"cell_type":"code","execution_count":null,"id":"a3d7b072","metadata":{"id":"a3d7b072","outputId":"0959cbeb-6d17-4437-8d5d-61a8cefcb929"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# To load the model, we need to define the model architecture and load the weights from the checkpoint file.\n","\n","model_path = './transformer_eng_spa.pth'\n","model.load_state_dict(torch.load(model_path))"]},{"cell_type":"code","execution_count":null,"id":"50740746","metadata":{"code_folding":[],"id":"50740746"},"outputs":[],"source":["def sentence_to_indices(sentence, word2idx):\n","    \"\"\"\n","    Converts a sentence to a list of token indices.\n","\n","    Parameters:\n","    sentence (str): The input sentence to be converted.\n","    word2idx (dict): A dictionary mapping words to their corresponding indices.\n","\n","    Returns:\n","    list: A list of token indices.\n","\n","    This function splits the input sentence into words and retrieves the corresponding indices from the word2idx dictionary.\n","    \"\"\"\n","    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n","\n","def indices_to_sentence(indices, idx2word):\n","    \"\"\"\n","    Converts a list of token indices to a sentence.\n","\n","    Parameters:\n","    indices (list): A list of token indices.\n","    idx2word (dict): A dictionary mapping token indices to words.\n","\n","    Returns:\n","    str: The sentence formed by the token indices.\n","\n","    This function iterates over the token indices and retrieves the corresponding words from the idx2word dictionary.\n","    \"\"\"\n","    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n","\n","def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n","    \"\"\"\n","    Translates a given sentence from English to Spanish using a translation model.\n","\n","    Parameters:\n","    model: The translation model to be used for generating translations.\n","    sentence (str): The input sentence in English to be translated.\n","    eng_word2idx (dict): A dictionary mapping English words to their corresponding indices.\n","    spa_idx2word (dict): A dictionary mapping Spanish indices to their corresponding words.\n","    max_len (int): The maximum length of the output sequence (default is MAX_SEQ_LEN).\n","    device (str): The device to run the model on (default is 'cpu').\n","\n","    Returns:\n","    str: The translated sentence in Spanish.\n","\n","    This function preprocesses the input sentence, converts it to indices, and feeds it into the model.\n","    It then iteratively generates the translated sentence token by token until the end-of-sequence token is produced\n","    or the maximum sequence length is reached. The function returns the translated sentence as a string.\n","    \"\"\"\n","\n","    # Set the model to evaluation mode and preprocess the input sentence, the input indices, and the input tensor.\n","    model.eval()\n","    sentence = preprocess_sentence(sentence)\n","    input_indices = sentence_to_indices(sentence, eng_word2idx)\n","    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n","\n","    # Initialize the target tensor with <sos> token\n","    tgt_indices = [spa_word2idx['<sos>']]\n","    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n","\n","    # Generate the translated sentence token by token\n","    with torch.no_grad():\n","        # Iterate until the end-of-sequence token is produced or the maximum sequence length is reached\n","        for _ in range(max_len):\n","            # Generate the next token\n","            output = model(input_tensor, tgt_tensor)\n","\n","            # Get the last token output and append it to the target tensor\n","            output = output.squeeze(0)\n","\n","            # Get the next token index\n","            next_token = output.argmax(dim=-1)[-1].item()\n","\n","            # Append the next token index to the target tensor\n","            tgt_indices.append(next_token)\n","\n","            # Break if the end-of-sequence token is produced\n","            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n","            if next_token == spa_word2idx['<eos>']:\n","                break\n","\n","    # Return the translated sentence in Spanish\n","    return indices_to_sentence(tgt_indices, spa_idx2word)"]},{"cell_type":"markdown","id":"a3dc8784","metadata":{"id":"a3dc8784"},"source":["Now, we will test the translator with some sentences in English and Spanish. This is a simple test to check if the translator is working correctly."]},{"cell_type":"code","execution_count":null,"id":"c2c0db72","metadata":{"code_folding":[15],"id":"c2c0db72","outputId":"cca19259-cc44-4e95-8968-f91c594e60d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input sentence: Hello, how are you?\n","Traducción: <sos> hola que tal <eos>\n","\n","Input sentence: I am learning artificial intelligence.\n","Traducción: <sos> estoy aprendiendo inteligencia artificial <eos>\n","\n","Input sentence: Artificial intelligence is great.\n","Traducción: <sos> la inteligencia artificial es grande <eos>\n","\n","Input sentence: Good night!\n","Traducción: <sos> buenas noches <eos>\n","\n"]}],"source":["def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n","    \"\"\"\n","    Evaluates translations for a list of input sentences using a given translation model.\n","\n","    Parameters:\n","    model: The translation model to be used for generating translations.\n","    sentences (list of str): A list of sentences in the source language (English) to be translated.\n","    eng_word2idx (dict): A dictionary mapping English words to their corresponding indices.\n","    spa_idx2word (dict): A dictionary mapping Spanish indices to their corresponding words.\n","    max_len (int): The maximum length of the sequences (default is MAX_SEQ_LEN).\n","    device (str): The device to run the model on (default is 'cpu').\n","\n","    Returns:\n","    None\n","\n","    This function iterates over each sentence in the input list, generates a translation using the model,\n","    and prints both the input sentence and its corresponding translation.\n","    \"\"\"\n","\n","    # Iterate over each input sentence and generate the corresponding translation\n","    for sentence in sentences:\n","        # Generate the translation using the translate_sentence function defined above\n","        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n","        print(f'Input sentence: {sentence}')\n","        print(f'Traducción: {translation}')\n","        print()\n","\n","# Example sentences to test the translator\n","test_sentences = [\n","    \"Hello, how are you?\",\n","    \"I am learning artificial intelligence.\",\n","    \"Artificial intelligence is great.\",\n","    \"Good night!\"\n","]\n","\n","# Assuming the model is trained and loaded\n","# Set the device to 'cpu' or 'cuda' as needed\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Evaluate translations\n","evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"]},{"cell_type":"markdown","id":"97f77011","metadata":{"id":"97f77011"},"source":["At this point, the most of the translation looks aceptable, but there are some sentences that are not translated as we expected. For example, the sentence \"I am a student\" is translated as \"Soy un estudiante\" but the correct translation is \"Yo soy un estudiante\". Also, the sentence \"I am a teacher\" is translated as \"Soy un profesor\" but the correct translation is \"Yo soy un profesor\"."]},{"cell_type":"markdown","id":"30cc072a","metadata":{"id":"30cc072a"},"source":["#### We included aditional sentences to test the translator"]},{"cell_type":"code","execution_count":null,"id":"4ceefe95","metadata":{"id":"4ceefe95","outputId":"2ad18618-a9fb-4744-bfd3-b2c88fb82a93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input sentence: Hello, can I ask you a question?\n","Traducción: <sos> hola puedo hacerte una pregunta <eos>\n","\n","Input sentence: I am learning how to build a transformer model.\n","Traducción: <sos> estoy aprendiendo a construir una modelo de internet <eos>\n","\n","Input sentence: The transformer model is a type of neural network.\n","Traducción: <sos> el modelo es un tipo de red de la red de red <eos>\n","\n","Input sentence: Goodbye!We are studying advanced natural language processing techniques.\n","Traducción: <sos> estamos estudiando diferentes tecnicas para que los poemas naturales <eos>\n","\n","Input sentence: Natural language processing is a fascinating field.\n","Traducción: <sos> el lenguaje natural es una lengua natural fascinante <eos>\n","\n","Input sentence: I'm a student at the university.\n","Traducción: <sos> soy estudiante en la universidad <eos>\n","\n","Input sentence: I live in Mexico.\n","Traducción: <sos> vivo en mexico <eos>\n","\n","Input sentence: We like to eat pizza on Fridays.\n","Traducción: <sos> nos gusta comer pizza los viernes <eos>\n","\n","Input sentence: The weather is nice today.\n","Traducción: <sos> hoy hace bueno <eos>\n","\n","Input sentence: The cat is sleeping on the sofa.\n","Traducción: <sos> el gato esta durmiendo en el sofa <eos>\n","\n","Input sentence: The dog is playing in the garden.\n","Traducción: <sos> el perro esta jugando en el jardin <eos>\n","\n","Input sentence: My favorite color is blue.\n","Traducción: <sos> mi color favorito es el azul <eos>\n","\n","Input sentence: The sky is clear and the sun is shining.\n","Traducción: <sos> el cielo esta brillando y despejado el cielo <eos>\n","\n","Input sentence: The moon is visible in the night sky.\n","Traducción: <sos> la luna es visible en el cielo nocturno <eos>\n","\n"]}],"source":["# Extra test sentences to evaluate the model further\n","extra_test_sentences = [\n","    \"Hello, can I ask you a question?\",\n","    \"I am learning how to build a transformer model.\",\n","    \"The transformer model is a type of neural network.\",\n","    \"Goodbye!\"\n","    \"We are studying advanced natural language processing techniques.\",\n","    \"Natural language processing is a fascinating field.\",\n","    \"I'm a student at the university.\",\n","    \"I live in Mexico.\",\n","    \"We like to eat pizza on Fridays.\",\n","    \"The weather is nice today.\",\n","    \"The cat is sleeping on the sofa.\",\n","    \"The dog is playing in the garden.\",\n","    \"My favorite color is blue.\",\n","    \"The sky is clear and the sun is shining.\",\n","    \"The moon is visible in the night sky.\",\n","]\n","\n","# Evaluate translations\n","evaluate_translations(model, extra_test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"]},{"cell_type":"markdown","id":"7d6cfdfd","metadata":{"id":"7d6cfdfd"},"source":["Clearly, we can observe that the translator has several limitations, such as the lack of vocabulary and the lack of training data. However, it is a good starting point to understand the transformer architecture."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}